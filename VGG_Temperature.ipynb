{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"VGG_Temperature.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":["MLio3SAbZCj3","RPiTo762ZCj3","Mkj3DZjpCdha","f3Bktt5PCdjh","BweFfNHSZClk","shC7RYg8ZClx"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"markdown","metadata":{"id":"-SvfKxAGCdhF"},"source":["# Modele VGG !"]},{"cell_type":"markdown","metadata":{"id":"VI1VQlwECdhG"},"source":["This time, you are going to use the [Oxford-IIIT Pet Dataset](http://www.robots.ox.ac.uk/~vgg/data/pets/) by [O. M. Parkhi et al., 2012](http://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf) which features 12 cat breeds and 25 dogs breeds."]},{"cell_type":"markdown","metadata":{"id":"h4dAhx8-CdhM"},"source":["##  Imports"]},{"cell_type":"code","metadata":{"id":"6oU4Z_DPCdhM"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import torch\n","import torch.nn as nn\n","import torchvision\n","from torchvision import models,transforms,datasets\n","import time\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A66_r51xCdhS"},"source":["torch.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KN3FFTFhHQyi"},"source":["import sys\n","sys.version"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tuej9DPjCdhX"},"source":["Check if GPU is available and if not change the [runtime](https://jovianlin.io/pytorch-with-gpu-in-google-colab/)."]},{"cell_type":"code","metadata":{"id":"t56d0zbFCdhY"},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","print('Using gpu: %s ' % torch.cuda.is_available())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dDLlOjT5Et4p"},"source":["## Downloading the data\n","\n","The data given on the website [Oxford-IIIT Pet Dataset](http://www.robots.ox.ac.uk/~vgg/data/pets/) is made of two files: `images.tar.gz` and `annotations.tar.gz`. We first need to download and decompress these files.\n","\n","Depending if you use google colab or your own computer, you can adapt the code below to choose where to store the data.\n","\n","To see where you are, you can use the standard unix comands:"]},{"cell_type":"code","metadata":{"id":"qEAr5HNeZCjm"},"source":["%pwd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JNQTp-x4ZCjo"},"source":["If you want to change to a directory to store your data:"]},{"cell_type":"code","metadata":{"id":"q5UwzcZKZCjp"},"source":["%cd #path"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ll1oQ5RbZCjr"},"source":["%pwd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rnn5pLK6EyJK"},"source":["%mkdir data\n","# the line below needs to be adapted if not running on google colab \n","%cd ./data/"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XkQKDwnfZCjv"},"source":["Now that you are in the right directory, you can download the data:"]},{"cell_type":"code","metadata":{"id":"BTobJ9vTE37J"},"source":["!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n","!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jEHXmG2vZCjy"},"source":["and uncompress it:"]},{"cell_type":"code","metadata":{"id":"XsMtmnCbCdhd"},"source":["!tar zxvf images.tar.gz\n","!tar zxvf annotations.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tJXlQ2ojZCj0"},"source":["Check that everything went correctly!"]},{"cell_type":"code","metadata":{"id":"nUlawYthZCj1"},"source":["%ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MLio3SAbZCj3"},"source":["## Warning\n","\n","If you are running this notebook on your own computer, you need to download the data only once. If you want to run this notebook a second time, you can safely skip this section and the section below as your dataset will be stored nicely on your computer.\n","\n","If you are running this notebook on google colab, you need to download the data and to do the data wrangling each time you are running this notebook as data will be cleared once you log off."]},{"cell_type":"markdown","metadata":{"id":"RPiTo762ZCj3"},"source":["## 1. Exercise: data wrangling\n","\n","You will first need to do a bit of [data wrangling](https://en.wikipedia.org/wiki/Data_wrangling) to organize your dataset in order to use the PyTorch `dataloader`.\n","\n","If you want to understand how the files are organized, have a look at the `README` file in the folder `annotations`.\n","\n","First, we need to split the dataset in a test set and train/validation set. For this, we can use the files `annotations/test.txt` and `annotations/trainval.txt` containing the names of images contained in the test and train/validation sets of the original paper."]},{"cell_type":"code","metadata":{"id":"EqNGCio0ZCj3"},"source":["!head annotations/test.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tejOC_TZZCj5"},"source":["!head annotations/trainval.txt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4O0BRsbmZCj7"},"source":["Above you see that the authors of the original paper made a partition of the dataset: `./images/Abyssinian_201.jpg` is in the test set while `./images/Abyssinian_100.jpg` is in the train/validation set and so on.\n","\n","BTW, it you wonder what Abyssinian means, it is explained [here](https://en.wikipedia.org/wiki/Abyssinian_cat)\n","\n","We first create two directories where we will put images form the test and trainval sets."]},{"cell_type":"code","metadata":{"id":"wn-tDnZXZCj7"},"source":["%mkdir test\n","%mkdir trainval"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5LwIlPrBZCj9"},"source":["%ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PgorOf_CZCkA"},"source":["Now it's your turn!\n","\n","All the images are in the `./images/` folder and you want to store the data according to the following structure:\n","```bash\n",".\n","├── test\n","|   └── Abyssinian # contains images of Abyssinian from the test set\n","|   └── Bengal # contains images of Bengal from the test set\n","|    ... \n","|   └── american_bulldog # contains images of american bulldog from the test set\n","|    ...\n","├── trainval\n","|   └── Abyssinian # contains images of Abyssinian from the trainval set\n","|   └── Bengal # contains images of Bengal from the trainval set\n","|    ...\n","|   └── american_bulldog # contains images of american bulldog from the trainval set\n","|    ...\n","```\n","\n","Note that all images wiht a name starting with a majuscule is a cat and all images with a name starting with a minuscule is a dog.\n","\n","So here is one way to achieve your task: you will read the `./annotations/test.txt` file line by line; from each line, you will extract the name of the corresponding file and then copy it from the `./images/filename_##.jpg` to `./test/filename/filename_##.jpg`, where `##` is a number.\n","\n","Then you'll do the same thing for `trainval.txt` file."]},{"cell_type":"markdown","metadata":{"id":"cAjydL_HZCkA"},"source":["Below is a little piece of code to show you how to open a file and read it line by line:"]},{"cell_type":"code","metadata":{"id":"-fiBsTj9ZCkA"},"source":["with open('./annotations/test.txt') as fp:\n","    line = fp.readline()\n","    while line:\n","        f,_,_,_ = line.split(' ')\n","        print(f)\n","        line = fp.readline()\n","        break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PLTktYgZZCkC"},"source":["In order to remove the `_201` in the example above, you can use the `re` [regular expression lib](https://docs.python.org/3.6/library/re.html) as follows:"]},{"cell_type":"code","metadata":{"id":"sQ9j71IsZCkD"},"source":["import re\n","pat = re.compile(r'_\\d')\n","res,_ = pat.split(f)\n","print(res)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1zVpVOpgZCkE"},"source":["This small piece of code might be useful:"]},{"cell_type":"code","metadata":{"id":"5YeBT3CbZCkF"},"source":["# create directory if it does not exist\n","def check_dir(dir_path):\n","    dir_path = dir_path.replace('//','/')\n","    os.makedirs(dir_path, exist_ok=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q0WzlkPUZCkG"},"source":["Some more hints:\n","- for moving files around you can use the `shutil` lib, see [here](https://docs.python.org/3.6/library/shutil.html#shutil.copy)\n","- you can use `os.path.join`\n","- have a look at python [f-string](https://cito.github.io/blog/f-strings/)"]},{"cell_type":"code","metadata":{"id":"qMHRHKe9ZCkH"},"source":["import shutil"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ciu2GLSZCkJ"},"source":["# Here your code for test\n","path_test = './test/'\n","with open('./annotations/test.txt') as fp:\n","    line = fp.readline()\n","    while line:\n","        f,_,_,_ = line.split(' ')\n","        res, _ = pat.split(f)\n","        path_f= os.path.join(path_test,res)\n","        check_dir(path_f)\n","        shutil.copy(f'./images/{f}.jpg', f'./test/{res}/{f}.jpg')\n","        #print(f)\n","        line = fp.readline()\n","        #break\n","   \n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G4jCU3QSZCkK"},"source":["# Here your code for train\n","\n","path_train = './trainval/'\n","with open('./annotations/trainval.txt') as fp:\n","    line = fp.readline()\n","    while line:\n","        f,_,_,_ = line.split(' ')\n","        res, _ = pat.split(f)\n","        path_f= os.path.join(path_train,res)\n","        check_dir(path_f)\n","        shutil.copy(f'./images/{f}.jpg', f'./trainval/{res}/{f}.jpg')\n","        #print(f)\n","        line = fp.readline()\n","        #break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mkj3DZjpCdha"},"source":["## Data processing"]},{"cell_type":"code","metadata":{"id":"iasXk_FKCdhy"},"source":["%cd .."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"anZGQpsFZCkP"},"source":["Now you are ready to redo what we did during lesson 1.\n","\n","Below, you give the path where the data is stored. If you are running this code on your computer, you should modifiy this cell."]},{"cell_type":"code","metadata":{"id":"MJRnJgGOCdh4"},"source":["data_dir = '/content/data/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U3lE0cvyCdh8"},"source":["```datasets``` is a class of the ```torchvision``` package (see [torchvision.datasets](http://pytorch.org/docs/master/torchvision/datasets.html)) and deals with data loading. It integrates a multi-threaded loader that fetches images from the disk, groups them in mini-batches and serves them continously to the GPU right after each _forward_/_backward_ pass through the network.\n","\n","Images needs a bit of preparation before passing them throught the network. They need to have all the same size $224\\times 224 \\times 3$ plus some extra formatting done below by the normalize transform (explained later)."]},{"cell_type":"code","metadata":{"id":"8t4vokNrF19p"},"source":["normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","\n","vgg_format = transforms.Compose([\n","                transforms.CenterCrop(224),\n","                transforms.ToTensor(),\n","                normalize,\n","            ])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l8LMReVECdh-"},"source":["dsets = {x: datasets.ImageFolder(os.path.join(data_dir, x), vgg_format)\n","         for x in ['trainval', 'test']}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pMh7kjEBCdiC"},"source":["os.path.join(data_dir,'trainval')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q8uKAPGAZCkW"},"source":["We now have 37 different classes."]},{"cell_type":"code","metadata":{"id":"m9ifn_R7CdiH"},"source":["dsets['trainval'].classes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TB6sTwFuCdiK"},"source":["dsets['trainval'].class_to_idx"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WefhjZb2CdiQ"},"source":["dset_sizes = {x: len(dsets[x]) for x in ['trainval', 'test']}\n","dset_sizes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SCLV1YgaCdiT"},"source":["dset_classes = dsets['trainval'].classes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zy52-XhACdiX"},"source":["The ```torchvision``` packages allows complex pre-processing/transforms of the input data (_e.g._ normalization, cropping, flipping, jittering). A sequence of transforms can be grouped in a pipeline with the help of the ```torchvision.transforms.Compose``` function, see [torchvision.transforms](http://pytorch.org/docs/master/torchvision/transforms.html)"]},{"cell_type":"code","metadata":{"id":"tWnJQiWgGP_R"},"source":["loader_train = torch.utils.data.DataLoader(dsets['trainval'], batch_size=64, shuffle=True,num_workers=6)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wN1BKHfDCdig"},"source":["loader_valid = torch.utils.data.DataLoader(dsets['test'], batch_size=5, shuffle=False,num_workers=6)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mQra6Q7CZCkj"},"source":["Check your dataloader and everything is doing fine"]},{"cell_type":"code","metadata":{"id":"Z4Be7lSLCdik"},"source":["count = 1\n","for data in loader_valid:\n","    print(count, end=',')\n","    if count == 1:\n","        inputs_try,labels_try = data\n","    count += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3BvxQqfzCdiq"},"source":["labels_try"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MLNsfqc8Cdis"},"source":["inputs_try.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vsaL21ouKwd9"},"source":["A small function to display images:"]},{"cell_type":"code","metadata":{"id":"346yl-gbcLLm"},"source":["def imshow(inp, title=None):\n","#   Imshow for Tensor.\n","    inp = inp.numpy().transpose((1, 2, 0))\n","    mean = np.array([0.485, 0.456, 0.406])\n","    std = np.array([0.229, 0.224, 0.225])\n","    inp = np.clip(std * inp + mean, 0,1)\n","    plt.imshow(inp)\n","    if title is not None:\n","        plt.title(title)\n","    plt.pause(0.001)  # pause a bit so that plots are updated"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IJbW-QzGCdiv"},"source":["# Make a grid from batch\n","out = torchvision.utils.make_grid(inputs_try)\n","\n","imshow(out, title=[dset_classes[x] for x in labels_try])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2HvFNWJICdiz"},"source":["# Get a batch of training data\n","inputs, classes = next(iter(loader_train))\n","\n","n_images = 8\n","\n","# Make a grid from batch\n","out = torchvision.utils.make_grid(inputs[0:n_images])\n","\n","imshow(out, title=[dset_classes[x] for x in classes[0:n_images]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LOvkYiROCdi7"},"source":["## 2. Exercise: modifying VGG Model"]},{"cell_type":"markdown","metadata":{"id":"BO5LAG4bCdi7"},"source":["The torchvision module comes with a zoo of popular CNN architectures which are already trained on [ImageNet](http://www.image-net.org/) (1.2M training images). When called the first time, if ```pretrained=True``` the model is fetched over the internet and downloaded to ```~/.torch/models```.\n","For next calls, the model will be directly read from there."]},{"cell_type":"code","metadata":{"id":"K9PsHjXgCdi9"},"source":["model_vgg = models.vgg16(pretrained=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p6QQhwruCdjI"},"source":["inputs_try , labels_try = inputs_try.to(device), labels_try.to(device)\n","\n","model_vgg = model_vgg.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"epMB0UF9CdjM"},"source":["outputs_try = model_vgg(inputs_try)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dOlx7YcPCdjO"},"source":["outputs_try"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OrweFfK9k0TW"},"source":["outputs_try.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f3Bktt5PCdjh"},"source":["### Modifying the last layer and setting the gradient false to all layers"]},{"cell_type":"code","metadata":{"id":"L8kr3-tjCdji"},"source":["print(model_vgg)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IXoMW73MCdjl"},"source":["We'll learn about what these different blocks do later in the course. For now, it's enough to know that:\n","\n","- Convolution layers are for finding small to medium size patterns in images -- analyzing the images locally\n","- Dense (fully connected) layers are for combining patterns across an image -- analyzing the images globally\n","- Pooling layers downsample -- in order to reduce image size and to improve invariance of learned features"]},{"cell_type":"markdown","metadata":{"id":"hA2f5FRuCdjm"},"source":["![vgg16](https://dataflowr.github.io/notebooks/Module1/img/vgg16.png)"]},{"cell_type":"markdown","metadata":{"id":"SK6lfAzfCdjn"},"source":["Here, our goal is to use the already trained model and just change the number of output classes. To this end we replace the last ```nn.Linear``` layer trained for 1000 classes to ones with 37 classes. In order to freeze the weights of the other layers during training, we set the field ```required_grad=False```. In this manner no gradient will be computed for them during backprop and hence no update in the weights. Only the weights for the 37-class layer will be updated."]},{"cell_type":"markdown","metadata":{"id":"qAr5tj2NKwfF"},"source":["PyTorch documentation for [LogSoftmax](https://pytorch.org/docs/stable/nn.html#logsoftmax)"]},{"cell_type":"code","metadata":{"id":"rQwRKKC-Cdjo"},"source":["#On charge le réseau VGG16 pré_entrainé\n","#On freeze les poids grace à requires_grad = False\n","#On modifie la dernière couche car nous avons 37 sorties\n","model_vgg = models.vgg16(pretrained=True)\n","for param in model_vgg.parameters():\n","    param.requires_grad = False\n","model_vgg.classifier._modules['6'] = nn.Linear(4096,37)\n","#model_vgg.classifier._modules['7'] = nn.LogSoftmax(dim=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jJ3OenJpCdjp"},"source":["print(model_vgg.classifier)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eisaEI7vZClA"},"source":["Once you modified the architecture of the network, do not forget to put in onto the device!"]},{"cell_type":"code","metadata":{"id":"9ITZFX2MCdju"},"source":["model_vgg = model_vgg.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fU9vWWT2CdkN"},"source":["## Training fully connected module"]},{"cell_type":"markdown","metadata":{"id":"qqp2u3IXCdkO"},"source":["### Creating loss function and optimizer\n","\n","PyTorch documentation for [NLLLoss](https://pytorch.org/docs/stable/nn.html#nllloss) and the [torch.optim module](https://pytorch.org/docs/stable/optim.html#module-torch.optim)"]},{"cell_type":"code","metadata":{"id":"oP1F4yb8CdkO"},"source":["criterion = nn.NLLLoss()\n","lr = 0.001\n","optimizer_vgg = torch.optim.SGD(model_vgg.classifier[6].parameters(),lr = lr)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tenuLj67CdkS"},"source":["### Training the model"]},{"cell_type":"code","metadata":{"id":"7nNAUibjCdkS"},"source":["def train_model(model,dataloader,size,epochs=1,optimizer=None):\n","    model.train()\n","    \n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        running_corrects = 0\n","        for inputs,classes in dataloader:\n","            inputs = inputs.to(device)\n","            classes = classes.to(device)\n","            outputs = F.log_softmax(model(inputs))\n","            loss = criterion(outputs,classes)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            _,preds = torch.max(outputs.data,1)\n","            # statistics\n","            running_loss += loss.data.item()\n","            running_corrects += torch.sum(preds == classes.data)\n","        epoch_loss = running_loss / size\n","        epoch_acc = running_corrects.data.item() / size\n","        print('Loss: {:.4f} Acc: {:.4f}'.format(\n","                     epoch_loss, epoch_acc))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Jts2jK1CdkV","scrolled":false},"source":["train_model(model_vgg,loader_train,size=dset_sizes['trainval'],epochs=20,optimizer=optimizer_vgg)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EpUTMK2OYcVh"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yr5GBTK1CzmC"},"source":["loader_valid2 = torch.utils.data.DataLoader(dsets['test'], batch_size=5, shuffle=False,num_workers=6)\n","loader_train2 = torch.utils.data.DataLoader(dsets['trainval'], batch_size=64, shuffle=False,num_workers=6)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BXJpV0K--DRX"},"source":["\n","import torch\n","from torch import nn, optim\n","from torch.nn import functional as F\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","\n","class ModelWithTemperature(nn.Module):\n","    \"\"\"\n","    A thin decorator, which wraps a model with temperature scaling\n","    model (nn.Module):\n","        A classification neural network\n","        NB: Output of the neural network should be the classification logits,\n","            NOT the softmax (or log softmax)!\n","    \"\"\"\n","    def __init__(self, model):\n","        super(ModelWithTemperature, self).__init__()\n","        self.model = model\n","        self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n","\n","    def forward(self, input):\n","        logits = self.model(input)\n","        return self.temperature_scale(logits)\n","\n","    def temperature_scale(self, logits):\n","        \"\"\"\n","        Perform temperature scaling on logits\n","        \"\"\"\n","        # Expand temperature to match the size of logits\n","        temperature = self.temperature.unsqueeze(1).expand(logits.size(0), logits.size(1))\n","        return logits / temperature\n","\n","    # This function probably should live outside of this class, but whatever\n","    def set_temperature(self, valid_loader, loss_optim, training = True):\n","        \"\"\"\n","        Tune the tempearature of the model (using the validation set).\n","        We're going to set it to optimize NLL.\n","        valid_loader (DataLoader): validation set loader\n","        \"\"\"\n","        self.cuda()\n","        nll_criterion = nn.CrossEntropyLoss().cuda()\n","        ece_criterion = _ECELoss().cuda()\n","        mce_criterion = _MCELoss().cuda()\n","\n","        # First: collect all the logits and labels for the validation set\n","        logits_list = []\n","        labels_list = []\n","        with torch.no_grad():\n","            for input, label in valid_loader:\n","                input = input.cuda()\n","                logits = self.model(input)\n","                logits_list.append(logits)\n","                labels_list.append(label)\n","            logits = torch.cat(logits_list).cuda()\n","            labels = torch.cat(labels_list).cuda()\n","\n","        # Calculate NLL and ECE before temperature scaling\n","        before_temperature_nll = nll_criterion(logits, labels).item()\n","        before_temperature_ece, before_avg_confidence_in_bin, before_accuracy_in_bin = ece_criterion(logits, labels)\n","        before_temperature_ece=before_temperature_ece.item()\n","        \n","        b1 = plt.bar( before_avg_confidence_in_bin, before_accuracy_in_bin, width = 0.05)\n","    \n","        print('Before temperature - NLL: %.3f, ECE/MCE: %.3f' % (before_temperature_nll, before_temperature_ece))\n","\n","        # Next: optimize the temperature w.r.t. NLL\n","        if training :\n","          optimizer = optim.LBFGS([self.temperature], lr=0.01, max_iter=50)\n","\n","          def eval():\n","              if loss_optim == \"ECE\" :\n","                loss,_,__ = ece_criterion(self.temperature_scale(logits), labels)\n","              if loss_optim == \"MCE\" :\n","                loss,_,__ = mce_criterion(self.temperature_scale(logits), labels)\n","              if loss_optim == \"NLL\" :\n","                loss= nll_criterion(self.temperature_scale(logits), labels)\n","              loss.backward()\n","              return loss\n","          optimizer.step(eval)\n","\n","        # Calculate NLL and ECE after temperature scaling\n","        after_temperature_nll = nll_criterion(self.temperature_scale(logits), labels).item()\n","        after_temperature_ece,  after_avg_confidence_in_bin, after_accuracy_in_bin = ece_criterion(self.temperature_scale(logits), labels)\n","        after_temperature_ece=after_temperature_ece.item()\n","        #print(after_avg_confidence_in_bin)\n","        #print(after_accuracy_in_bin)\n","        b2 = plt.bar( after_avg_confidence_in_bin, after_accuracy_in_bin, width = 0.05, color = 'green' , alpha = 0.5)\n","        plt.plot(np.arange(0,1+1/10,1/10),np.arange(0,1+1/10,1/10), color='red')\n","        plt.xlabel(\"proba\")\n","        plt.ylabel(\"accuracy\")\n","        plt.legend([b1, b2], ['avant temperature_scaling', 'après temperature_scaling'])\n","        if training :\n","          plt.title(\"Probabilité en fonction de l'accuracy, Entrainement, loss :  \"+loss_optim)\n","        else :\n","          plt.title(\"Probabilité en fonction de l'accuracy, Validation, loss :  \"+loss_optim)\n","        plt.show()\n","        print('Optimal temperature: %.3f' % self.temperature.item())\n","        print('After temperature - NLL: %.3f, ECE/MCE: %.3f' % (after_temperature_nll, after_temperature_ece))\n","\n","        return self\n","\n","\n","class _ECELoss(nn.Module):\n","    \"\"\"\n","    Calculates the Expected Calibration Error of a model.\n","    (This isn't necessary for temperature scaling, just a cool metric).\n","\n","    The input to this loss is the logits of a model, NOT the softmax scores.\n","\n","    This divides the confidence outputs into equally-sized interval bins.\n","    In each bin, we compute the confidence gap:\n","\n","    bin_gap = | avg_confidence_in_bin - accuracy_in_bin |\n","\n","    We then return a weighted average of the gaps, based on the number\n","    of samples in each bin\n","\n","    See: Naeini, Mahdi Pakdaman, Gregory F. Cooper, and Milos Hauskrecht.\n","    \"Obtaining Well Calibrated Probabilities Using Bayesian Binning.\" AAAI.\n","    2015.\n","    \"\"\"\n","    def __init__(self, n_bins=15):\n","        \"\"\"\n","        n_bins (int): number of confidence interval bins\n","        \"\"\"\n","        super(_ECELoss, self).__init__()\n","        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n","        self.bin_lowers = bin_boundaries[:-1]\n","        self.bin_uppers = bin_boundaries[1:]\n","\n","    def forward(self, logits, labels):\n","        softmaxes = F.softmax(logits, dim=1)\n","        confidences, predictions = torch.max(softmaxes, 1)\n","        accuracies = predictions.eq(labels)\n","\n","        ece = torch.zeros(1, device=logits.device)\n","        PROB = []\n","        ACCU = []\n","        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n","            # Calculated |confidence - accuracy| in each bin\n","            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n","            prop_in_bin = in_bin.float().mean()\n","            if prop_in_bin.item() > 0:\n","                accuracy_in_bin = accuracies[in_bin].float().mean()\n","                avg_confidence_in_bin = confidences[in_bin].mean()\n","                PROB +=[avg_confidence_in_bin.cpu().item()]\n","                ACCU +=[accuracy_in_bin.cpu().item()]\n","                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n","\n","        return (ece, PROB, ACCU)\n","\n","\n","\n","class _MCELoss(nn.Module):\n","    \"\"\"\n","    Calculates the Expected Calibration Error of a model.\n","    (This isn't necessary for temperature scaling, just a cool metric).\n","\n","    The input to this loss is the logits of a model, NOT the softmax scores.\n","\n","    This divides the confidence outputs into equally-sized interval bins.\n","    In each bin, we compute the confidence gap:\n","\n","    bin_gap = | avg_confidence_in_bin - accuracy_in_bin |\n","\n","    We then return a weighted average of the gaps, based on the number\n","    of samples in each bin\n","\n","    See: Naeini, Mahdi Pakdaman, Gregory F. Cooper, and Milos Hauskrecht.\n","    \"Obtaining Well Calibrated Probabilities Using Bayesian Binning.\" AAAI.\n","    2015.\n","    \"\"\"\n","    def __init__(self, n_bins=15):\n","        \"\"\"\n","        n_bins (int): number of confidence interval bins\n","        \"\"\"\n","        super(_MCELoss, self).__init__()\n","        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n","        self.bin_lowers = bin_boundaries[:-1]\n","        self.bin_uppers = bin_boundaries[1:]\n","\n","    def forward(self, logits, labels):\n","        softmaxes = F.softmax(logits, dim=1)\n","        confidences, predictions = torch.max(softmaxes, 1)\n","        accuracies = predictions.eq(labels)\n","\n","        ece = torch.zeros(1, device=logits.device)\n","        PROB = []\n","        ACCU = []\n","        MCE = []\n","        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n","            # Calculated |confidence - accuracy| in each bin\n","            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n","            prop_in_bin = in_bin.float().mean()\n","            if prop_in_bin.item() > 0:\n","                accuracy_in_bin = accuracies[in_bin].float().mean()\n","                avg_confidence_in_bin = confidences[in_bin].mean()\n","                PROB +=[avg_confidence_in_bin.cpu().item()]\n","                ACCU +=[accuracy_in_bin.cpu().item()]\n","                MCE += [torch.abs(avg_confidence_in_bin - accuracy_in_bin)]\n","\n","        return (max(MCE), PROB, ACCU)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QtUPmsZAqLfC"},"source":["\n","orig_model = model_vgg # create an uncalibrated model somehow\n","#valid_loader = loader_valid # Create a DataLoader from the SAME VALIDATION SET used to train orig_model\n","scaled_model = ModelWithTemperature(orig_model)\n","scaled_model = scaled_model\n","scaled_model = scaled_model.to(device)\n","### loss_optim : 'MCE', 'ECE' ou 'NLL'\n","print(loader_train2)\n","scaled_model.set_temperature(loader_train2, loss_optim = \"NLL\", training = True)\n","scaled_model.set_temperature(loader_valid2, loss_optim = \"NLL\", training = False)\n","\n","\n","orig_model = model_vgg # create an uncalibrated model somehow\n","#valid_loader = loader_valid # Create a DataLoader from the SAME VALIDATION SET used to train orig_model\n","scaled_model = ModelWithTemperature(orig_model)\n","scaled_model = scaled_model\n","scaled_model = scaled_model.to(device)\n","print(loader_train2)\n","scaled_model.set_temperature(loader_train2, loss_optim = \"ECE\", training = True)\n","scaled_model.set_temperature(loader_valid, loss_optim = \"ECE\", training = False)\n","\n","print(loader_train2)\n","\n","orig_model = model_vgg # create an uncalibrated model somehow\n","#valid_loader = loader_valid # Create a DataLoader from the SAME VALIDATION SET used to train orig_model\n","scaled_model = ModelWithTemperature(orig_model)\n","scaled_model = scaled_model\n","scaled_model = scaled_model.to(device)\n","scaled_model.set_temperature(loader_train2, loss_optim = \"MCE\", training = True)\n","scaled_model.set_temperature(loader_valid, loss_optim = \"MCE\", training = False)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YjjplkcdIc4i"},"source":[""],"execution_count":null,"outputs":[]}]}